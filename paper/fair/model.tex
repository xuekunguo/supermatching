%%==========================================================================
\section{Overview}
\label{sec:overview}

A tensor generalizes vectors and matrices to higher dimensions: a vector is a tensor of order one,
and a matrix is a tensor of order two. A higher-order tensor can be expressed as a multi-dimensional array~\cite{Kolda08}.
More formally, an $N$th-order tensor is an element of the tensor product of $N$ vector spaces, each with its own coordinate system.

Assume we are given two sets of feature points $P_1$ and $P_2$, with $N_1$ and $N_2$ points respectively.
From the $N$th-order tensor viewpoint, matching between these two feature sets can be represented by an \emph{assignment variable} $\boldsymbol{x}$.
The matching problem is equivalent to finding the optimal assignment tensor ${\boldsymbol{x}}^*=<x_{i_1},\cdots,x_{i_N}>
 \in \{0,1\}^{N}$, satisfying~\cite{Kolda08,Duchenne09}
\begin{eqnarray}
\label{equ:assigment}
  {\boldsymbol{x}}^* = &&\argmax_{\boldsymbol{x}}  \sum_{i_1,\cdots,i_N} \mathcal{T}_N(i_1,\cdots,i_N) x_{i_1}  \cdots\; x_{i_N}\\
 && \textrm{such that}\quad \sum_j x_{i_j} = 1.\nonumber
\end{eqnarray}
Here, $i_n \in \{i_1,\cdots ,i_N\}$ stands for an assignment in the $n^{th}$ dimension of the $N$ vector spaces.
Let all feature tuples for $P_1$ and $P_2$ be $F_1$ and $F_2$, then $\forall (f_{i_1}^1, \cdots, f_{i_N}^1)\in F_1$,
there is a matching to corresponding feature tuples in $F_2$.
For example, given a third-order tensor, $i_n \in \{1,2,3\}$,
each index could be expressed as $i_1=(f_{i_1}^1,f_{i_1}^2), i_2=(f_{i_2}^1,f_{i_2}^2), i_3=(f_{i_3}^1,f_{i_3}^2)$: pairs of potentially matched points.
The product $x_{i_1} \cdots\;x_{i_N}$ will be equal to $1$ if the points $(f_{i_1}^1, \cdots, f_{i_N}^1)$ are matched to the points $(f_{i_1}^2, \cdots, f_{i_N}^2)$,
and otherwise 0.
$\mathcal{T}_N(i_1,\cdots,i_N)$ is the affinity of the set of assignments $\{i_n\}_{n=1}^N$,
which is high if the features in tuple $(f_{i_1}^1, \cdots, f_{i_N}^1)$  have similar values to the features in the tuple $(f_{i_1}^2, \cdots, f_{i_N}^2)$, 
and their geometric constraints are similar.
Finally, the side constraint ensures that each point in $P_2$ can only match one point in $P_1$
Note that the size of $\mathcal{T}_N(i_1,\cdots,i_N)$ is ${(N_1N_2)}^N$.
In this paper, the affinity measures expressing similarity of feature tuples are stored using a supersymmetric tensor.

In the rest of the paper, we consider the one-to-many correspondence problem.
We assume that each point in $P_1$ is matched to exactly one point in $P_2$, but that the reverse is not necessarily true.
If we \emph{do} want to treat both datasets in the same way,
we can first match $P_1$ to $P_2$, then match $P_2$ to $P_1$, and then combine the matching results by taking their union or intersection.

From Equ.(\ref{equ:assigment}) we can see that there are four issues to be considered when using higher-order matching algorithms. How should we:
\begin{itemize}
\item organize and express the affinity measures $\mathcal{T}_N$ in a supersymmetric manner? (see Section~\ref{subsec:supersymtensor})
\item approximately solve the optimal higher-order assignment problem efficiently? (see Section~\ref{subsec:oursymmhopm})
\item define the affinity measure between two feature tuples? (see Section~\ref{subsec:potentials})
\item determine an appropriate sampling strategy to estimate the affinity tensor in a way which will give good matching accuracy (it is too large to compute fully)? (see Section~\ref{subsec:sampling})
\end{itemize}



