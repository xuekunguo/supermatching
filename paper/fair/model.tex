%%==========================================================================
\section{Overview}
\label{sec:overview}

A tensor generalizes vectors and matrices to higher dimensions: a vector is a tensor of order one,
and a matrix is a tensor of order two. A higher-order tensor can be expressed as a multi-dimensional array~\cite{Kolda08}.
More formally, an $N$th-order tensor is an element of the tensor product of $N$ vector spaces, each of which has its own coordinate system.

Assume we are given two sets of feature points $P$ and $Q$, with $N_1$ and $N_2$ points respectively.
From the $N$th-order tensor viewpoint,  matching between these two feature sets can be represented by an \emph{assignment variable} $\boldsymbol{x}$.
The matching problem is equivalent to finding the optimal assignment tensor ${\boldsymbol{x}}^*=<x_{i_1},\cdots,x_{i_N}>
 \in \{0,1\}^{N}$, satisfying~\cite{Kolda08,Duchenne09}
\begin{equation}
\label{equ:assigment}
  {\boldsymbol{x}}^* = \argmax_{\boldsymbol{x}}  \sum_{i_1,\cdots,i_N} \mathcal{T}_N(i_1,\cdots,i_N) x_{i_1}  \cdots\; x_{i_N}.
\end{equation}

Here, $i_n \in \{i_1,\cdots ,i_N\}$ stands for an assignment in the $n^{th}$ dimension of the $N$ vector spaces.
Let all feature tuples for $P$ and $Q$ be $F_1$ and $F_2$, then $\forall (f_{i_1}^1, \cdots, f_{i_N}^1)\in F_1$,
there is a corresponding matching to corresponding feature tuples in $F_2$.
For example, given a third-order tensor, $i_n \in \{1,2,3\}$,
each index could be expressed as $i_1=(f_{i_1}^1,f_{i_1}^2), i_2=(f_{i_2}^1,f_{i_2}^2), i_3=(f_{i_3}^1,f_{i_3}^2)$: pairs of potentially matched points.
The product $x_{i_1} \cdots\;x_{i_N}$ will be equal to $1$ if the points $(f_{i_1}^1, \cdots, f_{i_N}^1)$ are matched to the points $(f_{i_1}^2, \cdots, f_{i_N}^2)$,
and otherwise 0.
$\mathcal{T}_N(i_1,\cdots,i_N)$ is the affinity of the set of assignments $\{i_n\}_{n=1}^N$,
which will be high if the tuple of features $(f_{i_1}^1, \cdots, f_{i_N}^1)$  is similar to the tuple $(f_{i_1}^2, \cdots, f_{i_N}^2)$, and distances are similar.
Note that the size of $\mathcal{T}_N(i_1,\cdots,i_N)$ is ${(N_1N_2)}^N$.
In this paper, the affinity measures expressing similarity of feature tuples are stored using a supersymmetric tensor which is at the basis of our SuperMatching algorithm.

In the rest of the paper, we consider the one-to-many correspondence problem.
We assume that each point in $P$ is matched to exactly one point in $Q$, but that the reverse is not necessarily true.
If \emph{do} we want to treat both datasets in the same way,
we can first match $P$ to $Q$, then match $Q$ to $P$, and then combine the matching results by taking their union or intersection.

From Equ.(\ref{equ:assigment}) we can see that there are four issues to be considered when using higher-order matching algorithms. How should we:
\begin{itemize}
\item organize and express the affinity measures $\mathcal{T}_N$ in a supersymmetric manner? (see Section~\ref{subsec:supersymtensor})
\item approximately solve the optimal higher-order assignment problem efficiently? (see Section~\ref{subsec:oursymmhopm})
\item define the affinity measure between two feature tuples? (see Section~\ref{subsec:potentials})
\item determine an appropriate sampling strategy to estimate the affinity tensor in a way which will give good matching accuracy (it is too large to compute fully)? (see Section~\ref{subsec:sampling})
\end{itemize}



